---
title: "covid_recovery_analysis"
author: "Cary Ni"
date: "2023-03-23"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(mgcv)
library(earth)
library(gbm)
library(corrplot)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r}
# load the external dataset 
load("recovery.rdata")
# change the variable type based on the reference
index_factor = c(3, 4, 5,  9, 10, 13, 14, 15)
index_numer = c(2, 6, 7, 8, 11, 12)
dat[, index_factor]= lapply(dat[, index_factor], as.factor)
# extract 2000 samples for analysis
set.seed(2604)
dat <- dat[sample(1:10000, 2000),]
summary(dat)
# seperate training set and test set
set.seed(2023)
train_row = createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)
```

```{r}
# create covariates matrix for training and test
predictors_train = model.matrix(recovery_time ~ ., data = dat[train_row, -1])[, -1]
predictors_test = model.matrix(recovery_time ~ ., data = dat[-train_row, -1])[, -1]
# create response vector for training and test
response_train = dat[train_row, -1]$recovery_time
response_test = dat[-train_row, -1]$recovery_time
```

## Exploratory analysis and data visualization

### Visualize potential relationship between reponse variable and numeric predictors

```{r}
# simple visualization of the numeric data
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x = dat[, index_numer],
            y = dat$recovery_time, 
            plot = "scatter", 
            type = c("p", "smooth"),
            layout = c(3, 2))

```

### Visualize potential relationship between reponse variable and categorical predictors

```{r}
# simple visualization of the categorical data
par(mfrow=c(1,2))
myColors = c(rgb(0.1,0.1,0.7,0.5) , rgb(0.8,0.1,0.3,0.6), rgb(0.8,0.8,0.3,0.5),
             rgb(0.4,0.2,0.3,0.6))

boxplot(recovery_time ~ gender, data = dat, xlab = "gender", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ race, data = dat, xlab = "race", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ smoking, data = dat, xlab = "smoking status", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ hypertension, data = dat, xlab = "hypertension", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ diabetes, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ vaccine, data = dat, xlab = "vaccine", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ severity, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ study, data = dat, xlab = "study", col = myColors, ylim=c(0, 150))
```

### Correlation plot to check collinearity between covariates (based on training data) 

```{r}
cor(predictors_train) %>% corrplot(
  method = "circle", type = "full", 
  addCoef.col = 1, number.font =0.5,
  tl.col="black", tl.srt=90, tl.cex = 0.5,
  insig = "blank", diag=FALSE, number.cex = .3)
```

It can be seen from the scatterplots above that `SBP`, `LDL`, and `age` are the continuous variables linearly correlated to `recovery_time` while no apparent discrepancy is found when finding the association between `recovery_time` and the categorical variables of different levels. The correlation plot suggests that `SBP` is positively correlated to `hypertension` while the correlation among `bmi`, `weight`, and `height` are foreseen. In short, there is no obvious collinearity among most of the covariates (none of them exceed the threshold of 0.8). 

## Model Training

### Ordinary Least square

```{r}
# set train method
ctrl_1 = trainControl(method = "repeatedcv", number = 10, repeats = 5)
# build the linear least squared model with caret
set.seed(1)
lm_model = train(predictors_train, response_train, method = "lm", trControl = ctrl_1)
par(mfrow = c(2, 2))
plot(lm_model$finalModel)
```

While ordinary linear regression assumes linear relationship, normality of residuals, homoscedasticity, and independence of residual error terms, it can be seen from the diagnostic plots that the assumptions of normally distributed residuals and homoscedasticity are violated. Therefore, regularization methods and nonlinear methods should be introduced.

### Elastic net regression

```{r}
set.seed(1)
# build elastic net model with caret
elnet_model = train(predictors_train, response_train, 
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = seq(0, 1, length=21),
                                           lambda = exp(seq(-2, 8, length=50))),
                    trControl = ctrl_1)
myCol<- rainbow(21)
myPar <- list(superpose.symbol = list(col = myCol),
superpose.line = list(col = myCol))
plot(elnet_model, par.settings = myPar)
# show the best lambda and alpha combination with lowest cv rmse
elnet_model$bestTune
```

### Partial least squares

```{r}
set.seed(1)
pls_model = train(predictors_train, response_train,
                method = "pls",
                # 18 variables in total
                tuneGrid = data.frame(ncomp = 1:18), 
                trControl = ctrl_1,
                preProcess = c("center", "scale"))
ggplot(pls_model, highlight = TRUE) + 
  scale_x_continuous(breaks = seq(0,20,by=1))
```

While the assumptions of OLS are not met, parameter regularization/dimension reduction methods of the elastic net and partial least squares are used for model building. Through the process of 10 folds cross validation, the resulting elastic net model gives a penalty coefficient at 1, which is equivalent to the Lasso model. The partial least square model has 11 components when the cv error is minimized. 

### Generalized Additive Models (GAM)

```{r}
set.seed(1)
tune_Grid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE))
gam_model = train(predictors_train, response_train, 
                   method = "gam",
                  tuneGrid = tune_Grid,
                  trControl = ctrl_1)
par(mfrow = c(2, 2))
plot(gam_model$finalModel, shade = TRUE)
```

### Multivariate adaptive regression spline model (MARS)

```{r cache=TRUE}
set.seed(1)
# Set tuning parameters (20 as maximum number of terms taken since only 18 predictors are used)
mars_grid = expand.grid(degree = 1:3, nprune = 2:20)
# Fit MARS model
mars_model = train(predictors_train, response_train, 
                   method = "earth",
                   tuneGrid = mars_grid,
                   trControl = ctrl_1)
# Plot the model
plot(mars_model)
mars_model$bestTune
```

### K-Nearest Neighbors (KNN)

```{r}
set.seed(1)
knn_grid = expand.grid(k = seq(7, 15, by = 1))
knn_model = train(predictors_train, response_train,
                  method = "knn",
                  preProcess = c("center", "scale"),
                  trControl = ctrl_1,
                  tuneGrid = knn_grid)
knn_model$bestTune
```

### Generalized Boosted Regression

```{r}
set.seed(1)
# setting number of trees, depth, learning rate, and default leaves number
gbm_grid = expand.grid(
  n.trees = c(seq(50, 400, by = 50)),
  interaction.depth = c(1, 2), 
  shrinkage = 0.1,
  n.minobsinnode = 10
)
gbm_model = train(predictors_train, response_train,
                  method = "gbm",
                 preProcess = c("center", "scale"),
                 trControl = ctrl_1,
                 tuneGrid = gbm_grid,
                 verbose = FALSE)
plot(gbm_model)
```

### Models comparsion based on cross validation error

```{r}
# compare model performance through sampling method
resamp = resamples(list(
  lm = lm_model,
  enet = elnet_model,
  pls = pls_model,
  gam = gam_model,
  mars = mars_model,
  knn = knn_model,
  gbm = gbm_model
))
# plot resampling rmse
bwplot(resamp, metric = "RMSE")
summary(resamp)
```

## Results

Generalized boosted regression model is selected as the final model for its best training performance with lowest cross-validation root mean square error (23.78) among all models. The test mean squared error of this boosted regression model is 786.4. As shown by the final model, `bmi` is believed as the most important predictor, which accounts for more than 63% of the reduction to the loss function given this set of predictors. `height` and `weight` are second and third most important predictors because of their mathematical relationship to `bmi`. It can been seen from a partial dependence plot that the recovery time is relatively short for individuals with bmi from 23-30, which indicates that the obese patients with bmi greater than 30 tend to experience a remarkably long recovery period from Covid-19. `Vaccine`, another factor that draw the public attention, does help to shorten the recovery time  as shown by the partial dependence plot. On the other hand, the variables such as systolic blood pressure `(SBP)`, `race`, and `diabetes` generate little influence on the recovery time from Covid-19. 

```{r}
# get test mse
predict_value = predict(gbm_model, newdata = predictors_test)
test_mse = mean((predict_value - response_test)^2)
test_mse
par(mfrow = c(1, 1))
summary(gbm_model,
        cBars = 10,
        las = 2)
p1 = pdp::partial(gbm_model, pred.var = c("bmi"), 
                  grid.resolution = 10) %>% autoplot()
p2 = pdp::partial(gbm_model, pred.var = c("vaccine1"), 
                  grid.resolution = 10) %>% autoplot()
gridExtra::grid.arrange(p1, p2, nrow = 2)
```

## Conclusion

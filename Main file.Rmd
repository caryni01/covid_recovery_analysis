---
title: "covid_recovery_analysis"
author: "Cary Ni"
date: "2023-03-23"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(corrplot)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r}
# load the external dataset 
load("recovery.rdata")
# change the variable type based on the reference
index_factor = c(3, 4, 5,  9, 10, 13, 14, 15)
index_numer = c(2, 6, 7, 8, 11, 12)
dat[, index_factor]= lapply(dat[, index_factor], as.factor)
# extract 2000 samples for analysis
set.seed(2604)
dat <- dat[sample(1:10000, 2000),]
summary(dat)
# seperate training set and test set
set.seed(2023)
train_row = createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)
```

```{r}
# create covariates matrix for training and test
predictors_train = model.matrix(recovery_time ~ ., data = dat[train_row, -1])[, -1]
predictors_test = model.matrix(recovery_time ~ ., data = dat[-train_row, -1])[, -1]
# create response vector for training and test
response_train = dat[train_row, -1]$recovery_time
response_test = dat[-train_row, -1]$recovery_time
```

## Exploratory analysis and data visualization

### Visualize potential relationship between reponse variable and numeric predictors

```{r}
# simple visualization of the numeric data
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x = dat[, index_numer],
            y = dat$recovery_time, 
            plot = "scatter", 
            type = c("p", "smooth"),
            layout = c(3, 2))

```

### Visualize potential relationship between reponse variable and categorical predictors

```{r}
# simple visualization of the categorical data
par(mfrow=c(1,2))
myColors = c(rgb(0.1,0.1,0.7,0.5) , rgb(0.8,0.1,0.3,0.6), rgb(0.8,0.8,0.3,0.5),
             rgb(0.4,0.2,0.3,0.6))

boxplot(recovery_time ~ gender, data = dat, xlab = "gender", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ race, data = dat, xlab = "race", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ smoking, data = dat, xlab = "smoking status", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ hypertension, data = dat, xlab = "hypertension", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ diabetes, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ vaccine, data = dat, xlab = "vaccine", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ severity, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ study, data = dat, xlab = "study", col = myColors, ylim=c(0, 150))
```

### Correlation plot to check collinearity between covariates (based on training data) 

```{r}
cor(predictors_train) %>% corrplot(
  method = "circle", type = "full", 
  addCoef.col = 1, number.font =0.5,
  tl.col="black", tl.srt=90, tl.cex = 0.5,
  insig = "blank", diag=FALSE, number.cex = .3)
```

It can be seen from the scatterplots above that `SBP`, `LDL`, and `age` are the continuous variables linearly correlated to `recovery_time` while no apparent discrepancy is found when finding the association between `recovery_time` and the categorical variables of different levels. The correlation plot suggests that `SBP` is positively correlated to `hypertension` while the correlation among `bmi`, `weight`, and `height` are foreseen. In short, there is no obvious collinearity among most of the covariates (none of them exceed the threshold of 0.8). 

## Model Training

### Ordinary Least square

```{r}
# set train method
ctrl_1 = trainControl(method = "cv", number = 10)
# build the linear least squared model with caret
set.seed(1)
lm_model = train(predictors_train, response_train, method = "lm", trControl = ctrl_1)
par(mfrow = c(2, 2))
plot(lm_model$finalModel)
```

While ordinary linear regression assumes linear relationship, normality of residuals, homoscedasticity, and independence of residual error terms, it can be seen from the diagnostic plots that the assumptions of normally distributed residuals and homoscedasticity are violated. Therefore, regularization methods and nonlinear methods should be introduced.

### Elastic net regression

```{r}
set.seed(1)
# build elastic net model with caret
elnet_model = train(predictors_train, response_train, 
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = seq(0, 1, length=21),
                                           lambda = exp(seq(-2, 8, length=50))),
                    trControl = ctrl_1)
myCol<- rainbow(21)
myPar <- list(superpose.symbol = list(col = myCol),
superpose.line = list(col = myCol))
plot(elnet_model, par.settings = myPar)
# show the best lambda and alpha combination with lowest cv rmse
elnet_model$bestTune
```

### Partial least squares

```{r}
set.seed(1)
pls_model = train(predictors_train, response_train,
                method = "pls",
                # 18 variables in total
                tuneGrid = data.frame(ncomp = 1:18), 
                trControl = ctrl_1,
                preProcess = c("center", "scale"))
ggplot(pls_model, highlight = TRUE) + 
  scale_x_continuous(breaks = seq(0,20,by=1))
```

While the assumptions of OLS are not met, parameter regularization/dimension reduction methods of the elastic net and partial least squares are used for model building. Through the process of 10 folds cross validation, the resulting elastic net model gives a penalty coefficient at 1, which is equivalent to the Lasso model. The partial least square model has 12 components when the cv error is minimized. 


